data_path = '/home/nero/Documents/Estudos/Certificado_DataCamp/Exemplo_estudo_pratico/Data/toyota.csv'


# importing modules for data visualisation and exploration
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# loading the dataset and checking values
data = pd.read_csv(data_path)

# display the head for an view of the columns and initial values
# display the info to check the Dtype and general caracteristics
# describe the data to check an summary of the data caracteristics to better evaluate the cleaning methods
display(data.head(), data.info(), data.describe())

['model', 'transmission', 'fuelType', 'engineSize']
display(data['model'].unique(), data['transmission'].unique(), data['fuelType'].unique(), data['engineSize'].unique())


# cleaning the model column white spaces and changing to lower case
# for naming consistency
data['model'] = data['model'].str.strip().str.lower()

#changing the data types
categoricals = ['model', 'transmission', 'fuelType']
for col in categoricals:
    data[col] = data[col].astype('category')

# checking changes
data.info()


# importing seaborn for more plots
import seaborn as sns


sns.displot(data=data, y='model', kind='hist', hue='model', legend=False)


sns.displot(data=data, x='price', kind='kde')


# ploting the values from the diferent categories in boxplots to see the expected range of prices
fig, axes = plt.subplots(1,3,figsize=(20,5))
sns.boxplot(data=data, x='model',y='price',ax=axes[0]).set(title='Price range by Model')
sns.boxplot(data=data, x='transmission',y='price',ax=axes[1]).set(title='Price range by Transmission')
sns.boxplot(data=data, x='fuelType',y='price',ax=axes[2]).set(title='Price range by Fuel Type')
for ax in fig.axes:
    plt.sca(ax)
    plt.xticks(rotation=90);


ax_3 = sns.scatterplot(data=data, x='price', y='mileage', alpha=0.6, hue='model', legend='brief')
ax_3.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)


data[['mileage','price']].corr()


# Baseline model
from sklearn.linear_model import LinearRegression
# Normalize the model
from sklearn.preprocessing import StandardScaler
# Split data
from sklearn.model_selection import train_test_split, KFold
# Score the model based on cross validation
from sklearn.metrics import r2_score,mean_squared_error



dummy_vars = pd.get_dummies(data[['model','transmission','fuelType']], dtype=int)

model_data = pd.concat([data.drop(columns=['model','transmission','fuelType']), dummy_vars], axis=1)

model_data.head()


X = model_data.drop(columns=['price']) # Features
y = model_data['price'] # Target

scaler = StandardScaler()
# Alter only the needed columns 
X[['year','engineSize','mileage']] = scaler.fit_transform(X.loc[:,['year','engineSize','mileage']])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)


# creating the models
linreg = LinearRegression()

# fitting
linreg.fit(X_train, y_train)

# predictions
lin_y_pred = linreg.predict(X_test)

# scoring
lin_r2 = r2_score(y_test, lin_y_pred)
lin_rmse = np.sqrt(mean_squared_error(y_test, lin_y_pred))
print(f'linreg R2 score: {lin_r2:.4f} \nlinreg RMSE: {lin_rmse:.4f}')


# importing the model
from sklearn.tree import DecisionTreeRegressor
# using GridSearchCV to find the best parameters
from sklearn.model_selection import GridSearchCV

#params for grid search
params = {'max_depth':np.arange(1,21), 'min_samples_split':np.arange(2,5), 'min_samples_leaf':np.arange(2,6)}

# KFold 
kf = KFold(shuffle=True)
# creating the model
tree = DecisionTreeRegressor() #max_depth=12,min_samples_split=2,random_state=42
grid_search = GridSearchCV(tree, param_grid=params, cv=kf)
grid_search.fit(X_train, y_train)

# Selecting the best parameters for the DecisionTree
max_depth = grid_search.best_params_['max_depth']
min_samples_leaf = grid_search.best_params_['min_samples_leaf']
min_samples_split = grid_search.best_params_['min_samples_split']


# scoring the model based on the best parameters
tree = DecisionTreeRegressor(max_depth=max_depth,min_samples_split=min_samples_split,min_samples_leaf = min_samples_leaf, random_state=42)

# fitting the model
tree.fit(X_train, y_train)

# predictions
tree_y_pred = tree.predict(X_test)

# scoring
tree_r2 = r2_score(y_test, tree_y_pred)
tree_rmse = np.sqrt(mean_squared_error(y_test, tree_y_pred))
print(f'tree R2 score: {tree_r2:.4f} \ntree RMSE: {tree_rmse:.4f}')


# comparing scores

print('tree scores-> R2: {} ; RMSE:{} \n\nlinreg scores-> R2: {} ; RMSE: {}'.format(tree_r2,tree_rmse, lin_r2, lin_rmse))


lin_X = pd.DataFrame()
lin_X['diff'] = np.abs(y_test - lin_y_pred)
lin_X['sold'] = (lin_X['diff'] > (y_test * 0.3)).apply(lambda x: 'Not Sold' if x == True else 'Sold')

ax = lin_X['sold'].value_counts(normalize=True).plot.barh()
ax.bar_label((ax.containers[0]))
ax.spines[['right','top']].set_visible(False)
ax.set_title('Evaluating Linear Regression Model by KPI')


tree_X = pd.DataFrame()
tree_X['diff'] = np.abs(y_test - tree_y_pred)
tree_X['sold'] = (tree_X['diff'] > (y_test * 0.3)).apply(lambda x: 'Not Sold' if x == True else 'Sold')

ax = tree_X['sold'].value_counts(normalize=True).plot.barh()
ax.bar_label((ax.containers[0]))
ax.spines[['right','top']].set_visible(False)
ax.set_title('Evaluating Decision Tree Regression Model by KPI')
